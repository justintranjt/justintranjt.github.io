---
layout: post
title: "Aidoc Medical Internship Post-Mortem"
date: 2018-08-31
description: From Python WSGI server load-testing to developing secure servers for specialized HIPAA-regulated hospitals, I did it all while interning at Aidoc Medical.
image: /projects/wsgi
---
![]( /projects/wsgi.PNG )**

# What type of work did I perform as a software engineering intern at a Medical AI startup?

Due to the nature of working at a 2-year old startup with a development team sized at only 5 people (make that 6 as of the final weeks before I finished my internship), working as an intern meant *becoming* a regular member of the dev team.

Despite the challenges of working at a tech company made up of native Hebrew speakers, I was integrated into the team and began working on vital parts of their full-stack development environment. While tasked with an overarching project when there was no other work allocated to me, I continued to work on various parts of the Aidoc stack and became extremely familiar with Python web applications, production-grade server development, RESTful APIs, and informative error tracking.

Below is an in-depth look at the work I did while at Aidoc Medical for 10 weeks!

# De-Minifying JavaScript with Source Maps in Electron-Vue Applications

An important part of Aidoc's product is an application shipped to radiologists for use in their PACS setup. This application is written with [electron-vue](https://github.com/SimulatedGREG/electron-vue). Of course, when the application crashed or threw a nasty exception, we would have to discover its origins with an in-depth stacktrace using a service such as [Sentry.io](https://sentry.io).

However, Sentry doesn't play well with minified JavaScript code (which electron-vue serves up in its builds) and stacktraces become practically indecipherable.

Objective: Create a test electron-vue project and build a simple app that crashes and routes its crash data and stacktrace to Sentry.io. Note that all electron-vue files are packaged with electron-builder and minified with Babili upon compilation. Our goal is to generate a sourcemap using Webpack that can provide a full stacktrace to Sentry.io. The following solutions assume you follow the [Sentry docs regarding source maps](https://docs.sentry.io/clients/javascript/sourcemaps/).

The bundled and minified code is found in `[app_root_folder]/dist/electron`. All the Webpack configuration specifying settings are found in `[app_root_folder]/electron-vue` in the 3 webpack configuration files:


`webpack.main.config.js`
`webpack.renderer.config.js`
`webpack.web.config.js`

We'll be working with webpack.main.config.js for the purposes of this solution.

Importantly, Webpack is able to generate a source map during minification through the use of the `devtool` field. BUT none are generated for some reason which is the root of the problem. 
This important field's documentation can be found here. It's also specified in the Sentry docs as being a way to create source maps. While this file has no devtool field, `webpack.renderer.config.js` and `webpack.web.config.js` contain `cheap-module-eval-source-map` as the style of source mapping (note that those files don't generate a map either).

I found that changing the `devtool` field to a different mapping (we have 12 to choose from) was able to generate `.js.map` files in some cases. It turns out that the Babili-Webpack plugin is [bugged](https://github.com/webpack-contrib/babel-minify-webpack-plugin/issues/68) and can't generate some source map styles in its current version. 

Add `"webpack-sources": "1.0.1"` to the `devDependencies` field in `package.json`.

```javascript
const SentryWebpackPlugin = require('@sentry/webpack-plugin');

let mainConfig = {
  devtool: 'source-map',
  // other configuration
  plugins: [
    new SentryWebpackPlugin({
      release: 'release: 'process.env.NODE_ENV',
      include: './dist',
      ignoreFile: '.sentrycliignore',
      ignore: ['node_modules', 'webpack.config.js'],
      configFile: 'sentry.properties'
    })
  ]
};
```

Like so: ![]( /projects/wsgi.PNG )**

Remember to include with **source-map**. It is one one of the few source map styles that work with the bugged Babili-Webpack plugin.

This successfully generates a source map alongside the minified file. Right now, the minified file does not upload to Sentry successfully.

It looks like the map files are correctly uploading now. To get them to automatically upload, you must use the sentry-cli program with the command: `sentry-cli login`. If you can't access sentry-cli, it is found in `[app_root_folder]/node_modules/.bin`. 

You will login and copy and paste the auth token from the browser window that opens. Enter it and a .sentryclirc will be created in your application's root folder. It is now ready to ship source maps automatically now. Below I verified that they did upload automatically in the Releases -> Artifacts section.

image.png

However, the .js.map and .map files still don't show correctly on the Issues page and it looks as though others have run into similar problems before with different source map styles: 
image.png

Use this small program that reads your source map file and tests a mapping (remember to install `source-map` through `npm` first):
```
var fs        = require('fs'),
    path      = require('path'),
    sourceMap = require('source-map');

// file output by Webpack, Uglify, etc.
var GENERATED_FILE = path.join('.', '[YOUR_MAP_FILE_HERE]');

// line and column located in your generated file (e.g. source of your error
// from your minified file)
var GENERATED_LINE_AND_COLUMN = {line: 1, column: 1000};

var rawSourceMap = fs.readFileSync(GENERATED_FILE).toString();
var smc = new sourceMap.SourceMapConsumer(rawSourceMap);

var pos = smc.originalPositionFor(GENERATED_LINE_AND_COLUMN);

// should see something like:
// { source: 'original.js', line: 57, column: 9, name: 'myfunc' }
console.log(pos);
```

# Routing Sensitive Stacktraces Through HIPAA-Regulated Networks

We also host an Electron app that is installed on the computers of radiologists inside hospital networks. Of course, we had to handle similar crash data using Sentry.io. The twist here was having to use our own secure server approved by HIPAA regulations that could take any crash data from the Electron app and act as a reverse proxy to send the data to Sentry.io.

Objective: Configure an HTTPS route to a local Nginx server which will then send a query to Sentry.io. To do this we would need to create a self-signed SSL certificate. Configure the Electron app's Sentry's SDK to send data to this HTTPS port. Essentially, we are routing all crash data from the Electron app through a HTTPS Nginx server before using the server as a reverse proxy to send the data to the Sentry platform. This is required due to the limitations of the hospital network that only allow access to a virtual machine and no other internet access.

The following are a few of the important files vital to the process:

Electron
    `Main.js` (contains basic Electron app structure)
Nginx
    `Nginx.conf` (contains server configuration and HTTP to HTTPS redirection)
   ` Access.log` (log of all data to and from server)
Sentry
   ` Sentry.js` (contains DSN)

Our DSN is in the form `https://[PUBLIC_KEY]@[HOST]/[PROJECT_ID]`. The important aspect of this is that the host is sentry.io. Clearly we can’t use this configuration for our architecture where the hospital and its Electron app does not have access to the internet. So, we must route it through our localhost server running Nginx instead:

[image]

Note that our Nginx server must be started to begin this. Also note that we changed the host to be `localhost` (our Nginx server) and that the DSN is still HTTPS. Unfortunately, it turns out that Raven does not appreciate that our server uses a self-signed certificate. Raven throws an exception and our crash data fails to be routed through Nginx.

[image]

Sentry Possible Solution/Shortcomings:
Seemingly the only way around this error is to send the DSN without HTTPS and with HTTP instead. Everything works perfectly when the crash data is routed from Electron to our HTTP configured server and finally to the Sentry server (which is HTTPS) but this is obviously not secure. An alternative solution is to let our Nginx server config handle the redirection from HTTP to HTTPS instead (note that this works but is not secure)

Nginx.conf description:
    Lines 17-30 contain the server configuration that can be accessed through localhost or `http://localhost`. This does not utilize any certificate and is therefore unsecure. This is why lin5e 23 redirects all requests to `http://localhost` to `https://localhost` which is secured with a self-signed certificate. We want our data to be secure when making requests to and from the Nginx server which is why we use SSL or TLS.

    Lines 25-29 contain the proxy that takes the data sent from the Electron app crash and Sentry’s Raven client (which handles and reports all crash data in a form that can be read by Sentry). The location directive contains a regular expression that handles data sent from Raven in the query form: …/api/PROJECTID/store. This code was provided by a developer on the Sentry forum.

    Lines 33-65 contain the HTTPS configuration for our server. It is self-signed with an SSL certificate as seen in lines 37-41 and also password protected by a simple test username and password as seen in lines 43-45. Links to webpages and access to images stored in the server’s root are provided by location directives in lines 47-59.

    Lines 52-53 were used for testing the sending of HTTP requests to the server. Nginx servers hosting static pages do not allow POST requests (HTTP code 405) so a workaround was made in which GET requests with a request_body were sent to the server instead and 405 errors were converted to HTTP code 200 (success). This was all logged in access.log to ensure that HTTP requests and data sent to and from the server was making its way through Nginx effectively 

    NOW, our solution hinges on line 23 being either HTTP or HTTPS:
         This redirection works perfectly when simply querying the server for example at http://localhost and visiting the static webpage. However, this does not work when redirecting the DSN from Raven. Instead, we receive an error like so:

         [image]

        The DSN cannot be routed from HTTP to HTTPS through Nginx and eventually to the Sentry platform because of an undefined error. It must solely be through HTTP because Sentry does not want unsecured data anywhere during data transfer. Unfortunately we had to stick to this HTTP solution.

# Choosing the Perfect Python WSGI Server for Your Windows Environment in 2018

Another annoying aspect of working with tight restrictions due to HIPAA was the limited set of hardware that radiologists were able to work with. Nearly all the machines we developed for had to be Windows and as a result, building purely for Windows required using Windows-compatible tools at some points.

# Incidents Management Platform Solutions
    
When our virtual machines on Microsoft Azure and Amazon Web Services running Kubernetes containers for machine learning training crashed (and it happened often), we found errors to be difficult to trace. Even worse, it become difficult to pinpoint the solution when hundreds of metrics were taken into account at any moment. So, the solution was to take advantage of an incident management platform. I decided to choose between PagerDuty, VictorOps, and OpsGenie.

Below is some POC test code that I wrote for each of these incident management services. Along the way, I compared the feature-sets of each possible service and made a decision for our startup to prevent future failures that would go undetected.

![]( /projects/victorops.PNG )**

Having a working incident management solution that would integrate with our virtual machines performing algorithmic training on AWS (and old VMs on Azure), was essential to the efficiency and success of our machine learning platform. We were far too used to coming into work in the morning and realizing that our entire virutal machine had crashed early into the night.

With these solutions, we were able to create alerts in our cloud service of choice, integrate them to signal an incident in VictorOps (the service that I eventually chose and the one that the software engineering team took on), perform an analysis with integrataed Logz.io metrics from the ELK stack, and resolve the issue as soon as possible so that our precious server time was not wasted.

Note that this included writing my own integration between Microsoft Azure and VictorOps which I wrote about here: []().

The guiding principles for choosing between the three incidents management solutions boiled down to two key concerns:
    1. Possibility of integration with Microsofy Azure and AWS
    2. Ability to view raw data and metrics created by Logz.io and from our own queries

While this project required less code than one might imagine, choosing the appropriate service to integrate into our architecture was, of course, incredibly important to maintaining a working product for our customers.

[All code for the project can be found here](https://github.com/justintranjt/victorops-azure-integration).